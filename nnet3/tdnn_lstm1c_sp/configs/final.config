# This file was created by the command:
# steps/nnet3/xconfig_to_configs.py --xconfig-file exp/nnet3/tdnn_lstm1c_sp/configs/network.xconfig --config-dir exp/nnet3/tdnn_lstm1c_sp/configs/
# It contains the entire neural network.

input-node name=ivector dim=100
input-node name=input dim=40
component name=lda type=FixedAffineComponent matrix=exp/nnet3/tdnn_lstm1c_sp/configs/lda.mat
component-node name=lda component=lda input=Append(Offset(input, -2), Offset(input, -1), input, Offset(input, 1), Offset(input, 2), ReplaceIndex(ivector, t, 0))
component name=tdnn1.affine type=NaturalGradientAffineComponent input-dim=300 output-dim=520  max-change=0.75 l2-regularize=0.05
component-node name=tdnn1.affine component=tdnn1.affine input=lda
component name=tdnn1.relu type=RectifiedLinearComponent dim=520 self-repair-scale=1e-05
component-node name=tdnn1.relu component=tdnn1.relu input=tdnn1.affine
component name=tdnn1.batchnorm type=BatchNormComponent dim=520 target-rms=1.0
component-node name=tdnn1.batchnorm component=tdnn1.batchnorm input=tdnn1.relu
component name=tdnn2.affine type=NaturalGradientAffineComponent input-dim=1560 output-dim=520  max-change=0.75 l2-regularize=0.05
component-node name=tdnn2.affine component=tdnn2.affine input=Append(Offset(tdnn1.batchnorm, -1), tdnn1.batchnorm, Offset(tdnn1.batchnorm, 1))
component name=tdnn2.relu type=RectifiedLinearComponent dim=520 self-repair-scale=1e-05
component-node name=tdnn2.relu component=tdnn2.relu input=tdnn2.affine
component name=tdnn2.batchnorm type=BatchNormComponent dim=520 target-rms=1.0
component-node name=tdnn2.batchnorm component=tdnn2.batchnorm input=tdnn2.relu
##  Begin LTSM layer 'lstm1'
# Gate control: contains W_i, W_f, W_c and W_o matrices as blocks.
component name=lstm1.W_all type=NaturalGradientAffineComponent input-dim=650 output-dim=2080  max-change=1.5 l2-regularize=0.01 
# The core LSTM nonlinearity, implemented as a single component.
# Input = (i_part, f_part, c_part, o_part, c_{t-1}), output = (c_t, m_t)
# See cu-math.h:ComputeLstmNonlinearity() for details.
component name=lstm1.lstm_nonlin type=LstmNonlinearityComponent cell-dim=520 use-dropout=true  max-change=0.75 l2-regularize=0.01 
# Component for backprop truncation, to avoid gradient blowup in long training examples.
component name=lstm1.cr_trunc type=BackpropTruncationComponent dim=650 clipping-threshold=30.0 zeroing-threshold=15.0 zeroing-interval=20 recurrence-interval=3 scale=0.85
component name=lstm1.dropout_mask type=DropoutMaskComponent output-dim=3 dropout-proportion=0.0 
# Component specific to 'projected' LSTM (LSTMP), contains both recurrent
# and non-recurrent projections
component name=lstm1.W_rp type=NaturalGradientAffineComponent input-dim=520 output-dim=260  max-change=1.5 l2-regularize=0.01 
###  Nodes for the components above.
component-node name=lstm1.W_all component=lstm1.W_all input=Append(tdnn2.batchnorm, IfDefined(Offset(lstm1.r_trunc, -3)))
component-node name=lstm1.dropout_mask component=lstm1.dropout_mask input=lstm1.dropout_mask
component-node name=lstm1.lstm_nonlin component=lstm1.lstm_nonlin input=Append(lstm1.W_all, IfDefined(Offset(lstm1.c_trunc, -3)), lstm1.dropout_mask)
dim-range-node name=lstm1.c input-node=lstm1.lstm_nonlin dim-offset=0 dim=520
dim-range-node name=lstm1.m input-node=lstm1.lstm_nonlin dim-offset=520 dim=520
# lstm1.rp is the output node of this layer (if we're not including batchnorm)
component-node name=lstm1.rp component=lstm1.W_rp input=lstm1.m
dim-range-node name=lstm1.r input-node=lstm1.rp dim-offset=0 dim=130
# Note: it's not 100% efficient that we have to stitch the c
# and r back together to truncate them but it probably
# makes the deriv truncation more accurate .
component-node name=lstm1.cr_trunc component=lstm1.cr_trunc input=Append(lstm1.c, lstm1.r)
dim-range-node name=lstm1.c_trunc input-node=lstm1.cr_trunc dim-offset=0 dim=520
dim-range-node name=lstm1.r_trunc input-node=lstm1.cr_trunc dim-offset=520 dim=130
### End LSTM Layer 'lstm1'
component name=tdnn3.affine type=NaturalGradientAffineComponent input-dim=780 output-dim=520  max-change=0.75 l2-regularize=0.05
component-node name=tdnn3.affine component=tdnn3.affine input=Append(Offset(lstm1.rp, -3), lstm1.rp, Offset(lstm1.rp, 3))
component name=tdnn3.relu type=RectifiedLinearComponent dim=520 self-repair-scale=1e-05
component-node name=tdnn3.relu component=tdnn3.relu input=tdnn3.affine
component name=tdnn3.batchnorm type=BatchNormComponent dim=520 target-rms=1.0
component-node name=tdnn3.batchnorm component=tdnn3.batchnorm input=tdnn3.relu
component name=tdnn4.affine type=NaturalGradientAffineComponent input-dim=1560 output-dim=520  max-change=0.75 l2-regularize=0.05
component-node name=tdnn4.affine component=tdnn4.affine input=Append(Offset(tdnn3.batchnorm, -3), tdnn3.batchnorm, Offset(tdnn3.batchnorm, 3))
component name=tdnn4.relu type=RectifiedLinearComponent dim=520 self-repair-scale=1e-05
component-node name=tdnn4.relu component=tdnn4.relu input=tdnn4.affine
component name=tdnn4.batchnorm type=BatchNormComponent dim=520 target-rms=1.0
component-node name=tdnn4.batchnorm component=tdnn4.batchnorm input=tdnn4.relu
##  Begin LTSM layer 'lstm2'
# Gate control: contains W_i, W_f, W_c and W_o matrices as blocks.
component name=lstm2.W_all type=NaturalGradientAffineComponent input-dim=650 output-dim=2080  max-change=1.5 l2-regularize=0.01 
# The core LSTM nonlinearity, implemented as a single component.
# Input = (i_part, f_part, c_part, o_part, c_{t-1}), output = (c_t, m_t)
# See cu-math.h:ComputeLstmNonlinearity() for details.
component name=lstm2.lstm_nonlin type=LstmNonlinearityComponent cell-dim=520 use-dropout=true  max-change=0.75 l2-regularize=0.01 
# Component for backprop truncation, to avoid gradient blowup in long training examples.
component name=lstm2.cr_trunc type=BackpropTruncationComponent dim=650 clipping-threshold=30.0 zeroing-threshold=15.0 zeroing-interval=20 recurrence-interval=3 scale=0.85
component name=lstm2.dropout_mask type=DropoutMaskComponent output-dim=3 dropout-proportion=0.0 
# Component specific to 'projected' LSTM (LSTMP), contains both recurrent
# and non-recurrent projections
component name=lstm2.W_rp type=NaturalGradientAffineComponent input-dim=520 output-dim=260  max-change=1.5 l2-regularize=0.01 
###  Nodes for the components above.
component-node name=lstm2.W_all component=lstm2.W_all input=Append(tdnn4.batchnorm, IfDefined(Offset(lstm2.r_trunc, -3)))
component-node name=lstm2.dropout_mask component=lstm2.dropout_mask input=lstm2.dropout_mask
component-node name=lstm2.lstm_nonlin component=lstm2.lstm_nonlin input=Append(lstm2.W_all, IfDefined(Offset(lstm2.c_trunc, -3)), lstm2.dropout_mask)
dim-range-node name=lstm2.c input-node=lstm2.lstm_nonlin dim-offset=0 dim=520
dim-range-node name=lstm2.m input-node=lstm2.lstm_nonlin dim-offset=520 dim=520
# lstm2.rp is the output node of this layer (if we're not including batchnorm)
component-node name=lstm2.rp component=lstm2.W_rp input=lstm2.m
dim-range-node name=lstm2.r input-node=lstm2.rp dim-offset=0 dim=130
# Note: it's not 100% efficient that we have to stitch the c
# and r back together to truncate them but it probably
# makes the deriv truncation more accurate .
component-node name=lstm2.cr_trunc component=lstm2.cr_trunc input=Append(lstm2.c, lstm2.r)
dim-range-node name=lstm2.c_trunc input-node=lstm2.cr_trunc dim-offset=0 dim=520
dim-range-node name=lstm2.r_trunc input-node=lstm2.cr_trunc dim-offset=520 dim=130
### End LSTM Layer 'lstm2'
component name=tdnn5.affine type=NaturalGradientAffineComponent input-dim=780 output-dim=520  max-change=0.75 l2-regularize=0.05
component-node name=tdnn5.affine component=tdnn5.affine input=Append(Offset(lstm2.rp, -3), lstm2.rp, Offset(lstm2.rp, 3))
component name=tdnn5.relu type=RectifiedLinearComponent dim=520 self-repair-scale=1e-05
component-node name=tdnn5.relu component=tdnn5.relu input=tdnn5.affine
component name=tdnn5.batchnorm type=BatchNormComponent dim=520 target-rms=1.0
component-node name=tdnn5.batchnorm component=tdnn5.batchnorm input=tdnn5.relu
component name=tdnn6.affine type=NaturalGradientAffineComponent input-dim=1560 output-dim=520  max-change=0.75 l2-regularize=0.05
component-node name=tdnn6.affine component=tdnn6.affine input=Append(Offset(tdnn5.batchnorm, -3), tdnn5.batchnorm, Offset(tdnn5.batchnorm, 3))
component name=tdnn6.relu type=RectifiedLinearComponent dim=520 self-repair-scale=1e-05
component-node name=tdnn6.relu component=tdnn6.relu input=tdnn6.affine
component name=tdnn6.batchnorm type=BatchNormComponent dim=520 target-rms=1.0
component-node name=tdnn6.batchnorm component=tdnn6.batchnorm input=tdnn6.relu
##  Begin LTSM layer 'lstm3'
# Gate control: contains W_i, W_f, W_c and W_o matrices as blocks.
component name=lstm3.W_all type=NaturalGradientAffineComponent input-dim=650 output-dim=2080  max-change=1.5 l2-regularize=0.01 
# The core LSTM nonlinearity, implemented as a single component.
# Input = (i_part, f_part, c_part, o_part, c_{t-1}), output = (c_t, m_t)
# See cu-math.h:ComputeLstmNonlinearity() for details.
component name=lstm3.lstm_nonlin type=LstmNonlinearityComponent cell-dim=520 use-dropout=true  max-change=0.75 l2-regularize=0.01 
# Component for backprop truncation, to avoid gradient blowup in long training examples.
component name=lstm3.cr_trunc type=BackpropTruncationComponent dim=650 clipping-threshold=30.0 zeroing-threshold=15.0 zeroing-interval=20 recurrence-interval=3 scale=0.85
component name=lstm3.dropout_mask type=DropoutMaskComponent output-dim=3 dropout-proportion=0.0 
# Component specific to 'projected' LSTM (LSTMP), contains both recurrent
# and non-recurrent projections
component name=lstm3.W_rp type=NaturalGradientAffineComponent input-dim=520 output-dim=260  max-change=1.5 l2-regularize=0.01 
###  Nodes for the components above.
component-node name=lstm3.W_all component=lstm3.W_all input=Append(tdnn6.batchnorm, IfDefined(Offset(lstm3.r_trunc, -3)))
component-node name=lstm3.dropout_mask component=lstm3.dropout_mask input=lstm3.dropout_mask
component-node name=lstm3.lstm_nonlin component=lstm3.lstm_nonlin input=Append(lstm3.W_all, IfDefined(Offset(lstm3.c_trunc, -3)), lstm3.dropout_mask)
dim-range-node name=lstm3.c input-node=lstm3.lstm_nonlin dim-offset=0 dim=520
dim-range-node name=lstm3.m input-node=lstm3.lstm_nonlin dim-offset=520 dim=520
# lstm3.rp is the output node of this layer (if we're not including batchnorm)
component-node name=lstm3.rp component=lstm3.W_rp input=lstm3.m
dim-range-node name=lstm3.r input-node=lstm3.rp dim-offset=0 dim=130
# Note: it's not 100% efficient that we have to stitch the c
# and r back together to truncate them but it probably
# makes the deriv truncation more accurate .
component-node name=lstm3.cr_trunc component=lstm3.cr_trunc input=Append(lstm3.c, lstm3.r)
dim-range-node name=lstm3.c_trunc input-node=lstm3.cr_trunc dim-offset=0 dim=520
dim-range-node name=lstm3.r_trunc input-node=lstm3.cr_trunc dim-offset=520 dim=130
### End LSTM Layer 'lstm3'
component name=output.affine type=NaturalGradientAffineComponent input-dim=260 output-dim=2104  l2-regularize=0.01 max-change=1.5 param-stddev=0.0 bias-stddev=0.0
component-node name=output.affine component=output.affine input=lstm3.rp
component name=output.log-softmax type=LogSoftmaxComponent dim=2104
component-node name=output.log-softmax component=output.log-softmax input=output.affine
output-node name=output input=Offset(output.log-softmax, 5) objective=linear
